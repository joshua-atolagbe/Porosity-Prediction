{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "50fee284",
   "metadata": {},
   "source": [
    "# Libraries and Packages"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "ee0e5de9",
   "metadata": {},
   "outputs": [],
   "source": [
    "import lasio\n",
    "from os import path\n",
    "from glob import glob\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "import seaborn as sns\n",
    "\n",
    "import matplotlib as mpl\n",
    "from matplotlib import pyplot as plt\n",
    "\n",
    "import sklearn\n",
    "from sklearn.svm import SVR\n",
    "from sklearn.neural_network import MLPRegressor\n",
    "from sklearn.metrics import mean_absolute_error, r2_score\n",
    "from sklearn.model_selection import train_test_split\n",
    "from sklearn.preprocessing import RobustScaler\n",
    "from scipy.stats import probplot, pearsonr\n",
    "from warnings import filterwarnings\n",
    "from colorama import Fore\n",
    "\n",
    "%matplotlib inline\n",
    "%config InlineBackend.figure_format = 'retina'\n",
    "filterwarnings('ignore')\n",
    "\n",
    "#setting defaults\n",
    "pd.options.display.max_columns = 15\n",
    "pd.options.display.max_rows = 40\n",
    "# plt.style.use('ggplot')\n",
    "sns.set_style('darkgrid')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "da0c6814",
   "metadata": {},
   "outputs": [],
   "source": [
    "print(f'pandas version : {pd.__version__}')\n",
    "print(f'numpy version : {np.__version__}')\n",
    "print(f'scikit learn version : {sklearn.__version__}')\n",
    "print(f'seaborn version : {sns.__version__}')\n",
    "print(f'matplotlib version : {mpl.__version__}')\n",
    "print(f'lasio version : {lasio.version()}')"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "3eb69c11",
   "metadata": {},
   "source": [
    "# Functions"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "7d02e272",
   "metadata": {},
   "outputs": [],
   "source": [
    "#select columns to use in the dataframe\n",
    "col = ['DEPT:1','GR', 'RT', 'RHOB', 'SP', 'CALI', 'PHI']\n",
    "\n",
    "def fillArbitrary(dfs:pd.DataFrame) -> 'pd.Series|pd.DataFrame':\n",
    "    \"\"\"\n",
    "   fill in missing values in the dataframes with mean value\n",
    "    \"\"\"\n",
    "    df = dfs.copy()\n",
    "    df.dropna(axis='index', subset=['PHI'], inplace=True)\n",
    "    for c in df.columns:\n",
    "        df[c].fillna(df[c].mean(), inplace=True)\n",
    "        \n",
    "    depth = df['DEPTH'] #new depth range after dropping rows where nphi and sp are null\n",
    "    \n",
    "    df.drop(['DEPTH'], axis='columns', inplace=True) #dropping poorly correlating features\n",
    "    \n",
    "    return df, depth\n",
    "\n",
    "def processData(well:pd.DataFrame, cols:list, name:str) -> 'tuple|pd.DataFrame':\n",
    "    \n",
    "    '''\n",
    "    cleans and process the train dataframe for null values\n",
    "    \n",
    "    '''\n",
    "#     column = cols[:-2]\n",
    "    df = well.filter(cols, axis='columns')\n",
    "    \n",
    "    #filtering odd values in features \n",
    "    df['SP'] = np.where(df['SP'] < 0, np.nan, df['SP'])\n",
    "    df['PHI'] = np.where(df['PHI'] < 0, np.nan, df['PHI'])\n",
    "    #rename column\n",
    "    df = df.rename({'DEPT:1':'DEPTH'}, axis='columns')\n",
    "    df['RT'] = np.log10(df['RT']) #transforms the RT log to logarithmic scale\n",
    "    \n",
    "    #filter data to region where PHI readings are available\n",
    "    if name == 'WellB':\n",
    "        df = df[(df['DEPTH'] >= 2500) & (df['DEPTH'] <= 4200)]\n",
    "    else:\n",
    "        df = df[(df['DEPTH'] >= 3000) & (df['DEPTH'] <= 3600)]\n",
    "    \n",
    "    #calling the arbitrtary fill function\n",
    "    newdf, depth = fillArbitrary(df)\n",
    "    newdf['DEPTH'] = depth #adding new depth sequence to df\n",
    "    newdf = newdf.reset_index(drop=True).sort_values('DEPTH')\n",
    "\n",
    "    return newdf, df.shape\n",
    "\n",
    "def qq_plot_historesids(y_true, y_pred, name):\n",
    "    '''\n",
    "    quantile-quantile and histogram residual plots\n",
    "    '''\n",
    "    sns.set_style('darkgrid')\n",
    "    plt.subplots(nrows=1, ncols=2, figsize=(15, 5))\n",
    "    plt.suptitle(f'Metrics Plot for {name}', fontsize=20)\n",
    "    \n",
    "    plt.subplot(1, 2, 1)\n",
    "    resids = np.subtract(y_true.reshape(-1,1), y_pred.reshape(-1,1))\n",
    "    sns.distplot(resids)\n",
    "    plt.title(f'Histogram of residuals for {name}')\n",
    "    plt.xlabel('Residual value')\n",
    "    plt.ylabel('count')\n",
    "\n",
    "    plt.subplot(1, 2, 2)\n",
    "    resids = np.subtract(y_true, y_pred)\n",
    "    r2 = round(r2_score(y_true, y_pred), 2)\n",
    "    r2_adj = round(r2 - (6 - 1)/(y_true.shape[0] - 6) * (1 - r2), 2)\n",
    "    probplot(resids.flatten(), plot = plt)\n",
    "    plt.title(f'Residuals vs. Prediction for {name}')\n",
    "    plt.text(-2, 5, 'Adjusted R2 = ' + r2_adj.astype(str), fontsize=10, c='red')\n",
    "    plt.ylabel('Residuals')\n",
    "    plt.xlabel('Predicted Values')\n",
    "\n",
    "    \n",
    "def chatterjeeCorr(df:pd.DataFrame, x:str, y:str) -> float:\n",
    "    '''\n",
    "    implementing chatterjee method of calculating non-linear relationship between \n",
    "    independent and dependent variables\n",
    "    '''\n",
    "    dfs = df.copy()\n",
    "    dfs['x_rk'] = dfs[x].rank()\n",
    "    dfs['y_rk'] = dfs[y].rank()\n",
    "    dfs = dfs.sort_values('x_rk')\n",
    "    sum_term = dfs['y_rk'].diff().abs().sum()\n",
    "    chatt_corr = (1 - 3 * sum_term / (pow(dfs.shape[0], 2) - 1))\n",
    "    return chatt_corr\n",
    "\n",
    "\n",
    "def plotScatter(df:pd.DataFrame, *cols:tuple, y='PHI', rel='linear'):\n",
    "    \n",
    "    sns.set_style('whitegrid')\n",
    "    plt.subplots(nrows=2, ncols=3, figsize=(22, 11))\n",
    "    plt.suptitle(f'Scatter Plot Relationship', fontsize=20)\n",
    "    for i, x in enumerate(cols):\n",
    "        plt.subplot(2, 3, i+1)\n",
    "        if x == 'RT':\n",
    "            plt.title(y + ' vs. log' + x)\n",
    "        else:\n",
    "            plt.title(y + ' vs. ' + x)\n",
    "        sns.regplot(x=x, y=y, data=df, color='blue',\n",
    "                   fit_reg=True, scatter_kws={'s':10, 'alpha':0.5})\n",
    "        \n",
    "        if rel == 'linear':\n",
    "            r, p_val = pearsonr(df[x], df[y])\n",
    "        elif rel == 'non-linear':\n",
    "            r = chatterjeeCorr(df, x, y)\n",
    "        if x == 'DEPTH':\n",
    "            plt.text(2600, 60, f'R = {r:.2f}', bbox=dict(facecolor='red', alpha=0.5))\n",
    "        elif x=='CALI':\n",
    "            plt.text(10, 60, f'R = {r:.2f}', bbox=dict(facecolor='red', alpha=0.5))\n",
    "        else:\n",
    "            plt.text(2, 60, f'R = {r:.2f}', bbox=dict(facecolor='red', alpha=0.5))\n",
    "            \n",
    "\n",
    "def log_plot(logs, x1, x2, x3, x4, x5, x6, x7, well_name, top, bot):\n",
    "    \n",
    "    '''\n",
    "    plot well logs along with the predicted and ground truth of porosity\n",
    "    '''\n",
    "    sns.set_style('white')\n",
    "    # ztop = logs.DEPTH.min(); zbot=logs.DEPTH.max()\n",
    "    \n",
    "#     logs['litho number'] = [1 if i >= (logs['GR'].max() - logs['GR'].min())/2 else 0 for i in logs['GR'] ]\n",
    "  \n",
    "    # color = ['black', 'red', 'blue', 'green', 'cyan', 'purple', 'purple']\n",
    "    title = ['GR(gAPI)', 'log(RT)(ohm-m)', 'RHOB(g/cm3)', 'SP(mV)', 'CALI(in)', 'PHI(m3/m3)', 'Pred PHI(m3/m3)']\n",
    "    \n",
    "    f, ax = plt.subplots(nrows=1, ncols=len(title), figsize=(14, 7))\n",
    "    f.suptitle(f'Curve Plotting for {well_name}', fontsize=20, y=1.05)\n",
    "    \n",
    "    ax[0].plot(logs[x1], logs.DEPTH, 'black'); ax[1].semilogx(logs[x2], logs.DEPTH, 'black')\n",
    "    ax[2].plot(logs[x3], logs.DEPTH, 'black'); ax[3].plot(logs[x4], logs.DEPTH,'black')\n",
    "    ax[4].plot(logs[x5], logs.DEPTH, 'black'); ax[5].plot(logs[x6], logs.DEPTH, 'green')\n",
    "    ax[6].plot(logs[x7], logs.DEPTH,'blue')\n",
    "\n",
    "   \n",
    "    for i in range(len(ax)):\n",
    "        ax[i].set_ylim(top,bot); ax[i].invert_yaxis()\n",
    "        if i != 1:\n",
    "            ax[i].locator_params(axis='x', nbins=3)\n",
    "        ax[i].xaxis.label.set_color('black')\n",
    "        ax[i].grid(True); ax[i].tick_params(axis='x', colors='black')\n",
    "        ax[i].spines['top'].set_edgecolor('black'); ax[i].set_title(title[i], pad=15)\n",
    "        ax[i].spines[\"top\"].set_position((\"axes\", 1.02)); ax[i].xaxis.set_ticks_position(\"top\")\n",
    "        ax[i].xaxis.set_label_position(\"top\")\n",
    "    \n",
    "\n",
    "    ax[0].set_xlim(logs[x1].min(), logs[x1].max()); ax[1].set_xlim(logs[x2].min(), logs[x2].max())\n",
    "    ax[2].set_xlim(logs[x3].min(), logs[x3].max()); ax[3].set_xlim(logs[x4].min(), logs[x4].max())\n",
    "    ax[4].set_xlim(logs[x5].min(), logs[x5].max()); ax[5].set_xlim(logs[x6].min(), logs[x6].max())\n",
    "    ax[6].set_xlim(logs[x6].min(), logs[x6].max());\n",
    "    \n",
    "def plot_scatter(df, well_name):    \n",
    "    '''\n",
    "    plots the predicted and true poro reading on scatter plot\n",
    "    '''\n",
    "    sns.set_style('darkgrid')\n",
    "    plt.figure(figsize=(5,5.5))\n",
    "    sns.scatterplot('PHI', 'Predicted PHI', data=df)\n",
    "    # if well_name\n",
    "    plt.plot([10, 60], [10, 60], '--', c='black')\n",
    "    plt.xlabel('True PHI')\n",
    "    plt.ylabel('Pred PHI')\n",
    "    plt.title(f'{well_name}')"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "3f974e14",
   "metadata": {},
   "source": [
    "# Model Architecture"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "baa47cc9",
   "metadata": {},
   "outputs": [],
   "source": [
    "class BaseModel(object):\n",
    "    '''\n",
    "    class to porosity prediction\n",
    "    '''\n",
    "    def __init__(self, train, test):\n",
    "        '''\n",
    "        takes in the train and test dataframe\n",
    "        '''\n",
    "        self.train = train\n",
    "        self.test = test\n",
    "        \n",
    "    def __call__(self, plot=True):\n",
    "        \n",
    "        return self._fit_predict(plot=plot)\n",
    "    \n",
    "    def describe(self, train):\n",
    "        '''\n",
    "        returns the new summary statistics of the dataframe\n",
    "        '''\n",
    "        if train:\n",
    "            return self.train.describe()\n",
    "        else: \n",
    "            return self.test.describe()\n",
    "    \n",
    "    def _preprocess(self, train, test):\n",
    "        \n",
    "        '''\n",
    "        desc: splits the train dataframe into 80% train and 20% test and scales the data\n",
    "        returns: robust-scaled numpy data\n",
    "        '''\n",
    "\n",
    "        #getting feature and target\n",
    "        train_feature = (self.train.drop('PHI', axis='columns')).values;\n",
    "        test_feature = (self.test.drop('PHI', axis='columns')).values\n",
    "        trainlabel = np.array((self.train['PHI'])/100);\n",
    "        testlabel = np.array((self.test['PHI'])/100)\n",
    "        \n",
    "        ## Randomly sample cases to create independent training and validation data\n",
    "        np.random.seed(9988)\n",
    "        indx = range(train_feature.shape[0])\n",
    "        indx = train_test_split(indx, test_size = 0.15)\n",
    "        x_train = train_feature[indx[0],:]; y_train = np.ravel(trainlabel[indx[0]])\n",
    "        x_test = train_feature[indx[1],:]; y_test = np.ravel(trainlabel[indx[1]])\n",
    "\n",
    "        #standardization for train and test\n",
    "        scaler = RobustScaler(quantile_range=(25.0, 75.0)).fit(x_train)\n",
    "        x_train = scaler.transform(x_train); x_test = scaler.transform(x_test)\n",
    "        trainfeature = scaler.transform(train_feature); testfeature = scaler.transform(test_feature)\n",
    "        \n",
    "        return x_train, x_test, y_train, y_test, trainfeature, trainlabel, testfeature, testlabel\n",
    "    \n",
    "    def _fit_predict(self, plot=True):\n",
    "        \n",
    "        '''\n",
    "        fits models on the data and returns the stacked PHI results\n",
    "        '''\n",
    "        x_train, x_test, y_train, y_test, trainfeature, trainlabel, testfeature, testlabel = self._preprocess(self.train, self.test)\n",
    "        \n",
    "        model1 = SVR(C=10, epsilon=0.1, kernel='rbf', gamma='auto', tol=1e-10)\n",
    "        model2 = MLPRegressor(hidden_layer_sizes=(5,), activation='relu', solver='adam',\n",
    "                                   random_state=300, alpha=1, validation_fraction=0.3)\n",
    "        #fitting model and prediction \n",
    "        model1.fit(x_train, y_train); model2.fit(x_train, y_train)\n",
    "        \n",
    "        #validation on well one\n",
    "        print(Fore.BLUE + 'Validation Results on Well 1')\n",
    "        print('-'*50)\n",
    "        \n",
    "        y_pred1_1 = (model1.predict(trainfeature))*100 #svr prediction\n",
    "        y_pred1_2 = (model2.predict(trainfeature))*100 #mlp prediction\n",
    "        trainlabel = trainlabel*100 #denormalise the train label\n",
    "        final1 = (y_pred1_1 + y_pred1_2)/2 # avg. prediction        \n",
    "        print('The R2-score of SVR for Well 1 : %.2f' %r2_score(trainlabel, y_pred1_1))\n",
    "        print('The R2-score of MLP for Well 1 : %.2f' %r2_score(trainlabel, y_pred1_2))\n",
    "        print('The Abso. Error of SVR for Well 1 : %.2f' %mean_absolute_error(trainlabel, y_pred1_1))\n",
    "        print('The Abso. Error of MLP for Well 1 : %.2f' %mean_absolute_error(trainlabel, y_pred1_2))\n",
    "        print('The Average R2 Score for Well 1 : %.2f' %r2_score(trainlabel, final1))\n",
    "        print('The Average Abso. Error for Well 1 : %.2f' %mean_absolute_error(trainlabel, final1))\n",
    "        print('The Average R for Well 1 : %.2f' %np.sqrt(r2_score(trainlabel, final1)))\n",
    "\n",
    "        \n",
    "        print()\n",
    "        #validation on well two\n",
    "        print(Fore.YELLOW + 'Validation Results on Well 2')\n",
    "        print('-'*50)\n",
    "        y_pred2_1 = (model1.predict(testfeature))*100 #svr prediction\n",
    "        y_pred2_2 = (model2.predict(testfeature))*100 #mlp prediction\n",
    "        testlabel = testlabel*100 #denormalise the validation label\n",
    "        final2 = (y_pred2_1 + y_pred2_2)/2 # avg. prediction\n",
    "        print('The R2-score of SVR for Well 2 : %.2f' %r2_score(testlabel, y_pred2_1))\n",
    "        print('The R2-score of MLP for Well 2 : %.2f' %r2_score(testlabel, y_pred2_2))\n",
    "        print('The Abso. Error of SVR for Well 2 : %.2f' %mean_absolute_error(testlabel, y_pred2_1))\n",
    "        print('The Abso. Error of MLP for Well 2 : %.2f' %mean_absolute_error(testlabel, y_pred2_2))\n",
    "        print('The Average R2 Score for Well 2 : %.2f' %r2_score(testlabel, final2))\n",
    "        print('The Average Abso. Error for Well 2 : %.2f' %mean_absolute_error(testlabel, final2))\n",
    "        print('The Average R for Well 2 : %.2f' %np.sqrt(r2_score(testlabel, final2)))\n",
    "        \n",
    "        if plot:\n",
    "            #plot qq-plot and histogram of residuals\n",
    "            qq_plot_historesids(trainlabel, final1, 'WELLA#')\n",
    "            qq_plot_historesids(testlabel, final2, 'WELLB#')\n",
    "\n",
    "\n",
    "        return final1, final2"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "2ebbe7d5",
   "metadata": {},
   "source": [
    "# Data Loading"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "f46e12ca",
   "metadata": {},
   "outputs": [],
   "source": [
    "# get all paths and alphabetically ordered\n",
    "# paths = sorted(glob.glob(os.path.join(\"./\", \"*.LAS\")))\n",
    "paths = ['WELLB#.las', 'WELLA#.las']\n",
    "well_df = [0] * 2\n",
    "\n",
    "for i in range(len(paths)):\n",
    "    # read with lasio and convert to dataframe\n",
    "    df = (lasio.read(path.join('./', paths[i]))).df()\n",
    "\n",
    "    well_df[i] = df.reset_index()\n",
    "\n",
    "well1, well2 = well_df\n",
    "\n",
    "well1, shape1=processData(well1, col, name='WellA') #validation\n",
    "well2, shape2=processData(well2, col, name='WellB') #training\n",
    "\n",
    "train = well2\n",
    "test = well1"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "e9287332",
   "metadata": {},
   "source": [
    "# Correlation"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "f7d6e80b",
   "metadata": {},
   "outputs": [],
   "source": [
    "plt.figure(figsize=((13, 7)))\n",
    "corr = train.select_dtypes('float64').corr()\n",
    "sns.heatmap(corr, annot=True, vmin=0, vmax=1, linewidths=0.01)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "7f6e09fd",
   "metadata": {},
   "outputs": [],
   "source": [
    "plotScatter(train, 'DEPTH', 'GR', 'RHOB', 'CALI', 'SP', 'RT', rel='non-linear') #chatterjee\n",
    "plotScatter(train, 'DEPTH', 'GR', 'RHOB', 'CALI', 'SP', 'RT', rel='linear') #pearson's"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "f57b2a61",
   "metadata": {},
   "source": [
    "# Prediction"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "0e6d6939",
   "metadata": {},
   "outputs": [],
   "source": [
    "basemodel = BaseModel(train, test)\n",
    "well1_pred, well2_pred = basemodel(plot=True)\n",
    "\n",
    "#add predicted results to the original dataframes\n",
    "train['Predicted PHI'] = well1_pred\n",
    "test['Predicted PHI'] = well2_pred"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "a20b6902",
   "metadata": {},
   "source": [
    "# Plots"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "50bb3ac1",
   "metadata": {},
   "outputs": [],
   "source": [
    "#validation scatter plots\n",
    "plot_scatter(train, 'WellA#')\n",
    "plot_scatter(test, 'WellB#')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "77e07125",
   "metadata": {},
   "outputs": [],
   "source": [
    "#log plots\n",
    "log_plot(train, 'GR', 'RT', 'RHOB', 'SP', 'CALI', 'PHI', 'Predicted PHI', well_name='WellA#',top=2500, bot=2700)\n",
    "log_plot(test, 'GR', 'RT', 'RHOB', 'SP', 'CALI', 'PHI', 'Predicted PHI', well_name='WellB#',top=3100, bot=3200)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.3"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
